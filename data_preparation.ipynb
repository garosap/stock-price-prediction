{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f582c5",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preparation\n",
    "\n",
    "This notebook covers the initial stages of the data pipeline for our machine learning project on stock price prediction using LSTM and Transformer models. It includes the setup for data extraction, transformation and loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aef2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c576787",
   "metadata": {},
   "source": [
    "\n",
    "## ETL Class - Extract, transform, and load \n",
    "\n",
    "\n",
    "The `ETL` class is a critical component in our machine learning pipeline, designed to manage the data lifecycle for stock price prediction models. It automates the steps from data extraction through to preparation for modeling, ensuring data is ready for analysis and prediction. Below, we outline the key functionalities of the class:\n",
    "\n",
    "### Key Functionalities\n",
    "\n",
    "- **Data Extraction**: Retrieves historical stock price data from the Yahoo Finance API, focusing on the closing prices for the specified stock ticker.\n",
    "\n",
    "- **Data Transformation**: Processes the data to fit the specific requirements of our machine learning models. This includes reshaping the data into sequences that represent consistent time intervals, ensuring that the models can recognize patterns over time.\n",
    "\n",
    "- **Data Loading**: Splits the data into training and testing datasets. The training set is used to teach our models how to predict future prices, while the test set is used to evaluate the accuracy of these predictions.\n",
    "\n",
    "- **Supervised Learning Conversion**: Transforms the time series data into a format suitable for supervised learning, where the model can learn from past data to predict future outcomes.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The class is implemented with methods that handle each stage of the data handling process:\n",
    "\n",
    "- **Initialization**: Sets up the class with parameters like the stock ticker, the size of the test dataset, and the details of how data should be segmented.\n",
    "\n",
    "- **Data Splitting**: Divides the data into training and testing sets based on a specified proportion.\n",
    "\n",
    "- **Reshaping and Windowing**: Adjusts the structure of the data arrays to meet the input requirements of predictive models, organizing the data into meaningful segments.\n",
    "\n",
    "- **Complete ETL Execution**: Runs all steps from data extraction to loading in sequence, delivering data that's ready for modeling.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "This class streamlines the preparation of financial time series data, enabling efficient and effective model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "980f1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETL:\n",
    "    \"\"\"\n",
    "    ticker: str\n",
    "    period: string\n",
    "    test_size: float betwee 0 and 1\n",
    "    n_input: int\n",
    "    timestep: int\n",
    "    Extracts data for stock with ticker `ticker` from yf api,\n",
    "    splits the data into train and test sets by date,\n",
    "    reshapes the data into np.array of shape [#weeks, 5, 1],\n",
    "    converts our problem into supervised learning problem.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, test_size=0.2, period='max', n_input=5, timestep=5) -> None:\n",
    "        self.ticker = ticker\n",
    "        self.period = period\n",
    "        self.test_size = test_size\n",
    "        self.n_input = n_input\n",
    "        self.df = self.extract_historic_data()\n",
    "        self.timestep = timestep\n",
    "        self.train, self.test = self.etl()\n",
    "        self.X_train, self.y_train = self.to_supervised(self.train)\n",
    "        self.X_test, self.y_test = self.to_supervised(self.test)\n",
    "\n",
    "    def extract_historic_data(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Gets historical data from yf api.\n",
    "        \"\"\"\n",
    "        t = yf.Ticker(self.ticker)\n",
    "        history = t.history(period=self.period)\n",
    "        return history.Close\n",
    "\n",
    "    def split_data(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Splits our pd.Series into train and test series with\n",
    "        test series representing test_size * 100 % of data.\n",
    "        \"\"\"\n",
    "        data = self.extract_historic_data()\n",
    "        if len(data) != 0:\n",
    "            train_idx = round(len(data) * (1-self.test_size))\n",
    "            train = data[:train_idx]\n",
    "            test = data[train_idx:]\n",
    "            train = np.array(train)\n",
    "            test = np.array(test)\n",
    "            return train[:, np.newaxis], test[:, np.newaxis]\n",
    "        else:\n",
    "            raise Exception('Data set is empty, cannot split.')\n",
    "\n",
    "    def window_and_reshape(self, data) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformats data into shape our model needs,\n",
    "        namely, [# samples, timestep, # feautures]\n",
    "        samples\n",
    "        \"\"\"\n",
    "        NUM_FEATURES = 1\n",
    "        samples = int(data.shape[0] / self.timestep)\n",
    "        result = np.array(np.array_split(data, samples))\n",
    "        return result.reshape((samples, self.timestep, NUM_FEATURES))\n",
    "\n",
    "    def transform(self, train, test) -> np.array:\n",
    "        train_remainder = train.shape[0] % self.timestep\n",
    "        test_remainder = test.shape[0] % self.timestep\n",
    "        if train_remainder != 0 and test_remainder != 0:\n",
    "            train = train[train_remainder:]\n",
    "            test = test[test_remainder:]\n",
    "        elif train_remainder != 0:\n",
    "            train = train[train_remainder:]\n",
    "        elif test_remainder != 0:\n",
    "            test = test[test_remainder:]\n",
    "        return self.window_and_reshape(train), self.window_and_reshape(test)\n",
    "\n",
    "    def etl(self) -> 'tuple[np.array, np.array]':\n",
    "        \"\"\"\n",
    "        Runs complete ETL\n",
    "        \"\"\"\n",
    "        train, test = self.split_data()\n",
    "        return self.transform(train, test)\n",
    "    \n",
    "\n",
    "    def to_supervised(self, train, n_out=5) -> tuple:\n",
    "        \"\"\"\n",
    "        Converts our time series prediction problem to a\n",
    "        supervised learning problem.\n",
    "        \"\"\"\n",
    "        # flatted the data\n",
    "        data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "        X, y = [], []\n",
    "        in_start = 0\n",
    "        # step over the entire history one time step at a time\n",
    "        for _ in range(len(data)):\n",
    "            # define the end of the input sequence\n",
    "            in_end = in_start + self.n_input\n",
    "            out_end = in_end + n_out\n",
    "            # ensure we have enough data for this instance\n",
    "            if out_end <= len(data):\n",
    "                x_input = data[in_start:in_end, 0]\n",
    "                x_input = x_input.reshape((len(x_input), 1))\n",
    "                X.append(x_input)\n",
    "                y.append(data[in_end:out_end, 0])\n",
    "                # move along one time step\n",
    "                in_start += 1\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "        \n",
    "    def save_data(self, folder=\"datasets\"):\n",
    "        \"\"\"\n",
    "        Saves the data to pickle files.\n",
    "        \"\"\"\n",
    "        with open(folder + \"/df.pkl\", \"wb\") as f:\n",
    "            pkl.dump(self.df, f)\n",
    "            \n",
    "        with open(folder + \"/train.pkl\", \"wb\") as f:\n",
    "            pkl.dump(self.train, f)\n",
    "            \n",
    "        with open(folder + \"/test.pkl\", \"wb\") as f:\n",
    "            pkl.dump(self.test, f)\n",
    "    \n",
    "        with open(folder + \"/X_train.pkl\", \"wb\") as f:\n",
    "            pkl.dump(self.X_train, f)\n",
    "            \n",
    "        with open(folder + \"/y_train.pkl\", \"wb\") as f:\n",
    "            pkl.dump(self.y_train, f)\n",
    "            \n",
    "        with open(folder + \"/X_test.pkl\", \"wb\") as f:\n",
    "            pkl.dump(self.X_test, f)\n",
    "            \n",
    "        with open(folder + \"/y_test.pkl\", \"wb\") as f:\n",
    "            pkl.dump(self.y_test, f)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca689a7f",
   "metadata": {},
   "source": [
    "\n",
    "## Data Loading and Preprocessing\n",
    "\n",
    "Here we load and preprocess our data using the `ETL` class defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a33f7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ETL('AAPL')\n",
    "\n",
    "# store data\n",
    "data.save_data('datasets')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
